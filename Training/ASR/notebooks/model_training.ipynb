{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTI5sLw3WWBA",
        "outputId": "d0edddd5-eec8-42f3-a580-54c6c3f4f149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5LPD3JsZtmT",
        "outputId": "7f067138-4eb0-48dc-cd0d-bf8d2c978ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 14 09:55:32 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A63beGG_Zvox",
        "outputId": "165c96d0-4b89-4991-ba6e-25f212a3729b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "hLRrHseN61L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "device_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wjIiEXlV6xYd",
        "outputId": "bc404ab3-a292-4ef4-f2ff-939e4904a204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning Repo"
      ],
      "metadata": {
        "id": "UacoO9ysXZza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/manishdhakal/ASR-Nepali-using-CNN-BiLSTM-ResNet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TBqs5YQXT8c",
        "outputId": "cba9b974-62b9-47f6-ba26-7c93d84ddabc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ASR-Nepali-using-CNN-BiLSTM-ResNet'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 86 (delta 17), reused 80 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (86/86), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Dataset"
      ],
      "metadata": {
        "id": "2G9xospPXbw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content\n",
        "# !mkdir ~/.kaggle \n",
        "# !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json \n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhf5d25CXXr3",
        "outputId": "15b9a41c-3498-4c3f-b85d-f4e05fa4e673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download anishshilpakar/asr-cnn-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYFlCOTmXtdp",
        "outputId": "0a003230-3f18-4a25-95d7-226da1925ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading asr-cnn-dataset.zip to /content\n",
            "100% 6.88G/6.88G [06:12<00:00, 23.4MB/s]\n",
            "100% 6.88G/6.88G [06:12<00:00, 19.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting"
      ],
      "metadata": {
        "id": "FqisW9jiX1h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# %cd /content\n",
        "# !unzip asr-cnn-dataset"
      ],
      "metadata": {
        "id": "es24lKOGX1GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cd /content\n",
        "!unzip /content/drive/MyDrive/audio.zip"
      ],
      "metadata": {
        "id": "XQFvtBej-5yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install edit_distance \n",
        "!pip install datasets\n",
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "D2YsZuccYUCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK3U_uI3Ems0",
        "outputId": "d4691bd3-989f-4c8b-95bb-99e936b26212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 11:12:26.865577: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "/content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py:19: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  wer_metric = load_metric(\"wer\")\n",
            "/device:GPU:0\n",
            "Model defined âœ… âœ… âœ… âœ…\n",
            "\n",
            "Loading data.....\n",
            "There are 22695 files\n",
            "Data loaded âœ… âœ… âœ… âœ…\n",
            "And It took 28.128363132476807 seconds\n",
            "\n",
            "Cleaning the audio files.....\n",
            "Audio files cleaned âœ… âœ… âœ… âœ…\n",
            "And It took 18.67191505432129 seconds\n",
            "\n",
            "Generating mfcc features.....\n",
            "MFCC features generated âœ… âœ… âœ… âœ…\n",
            "And It took 528.8424408435822 seconds\n",
            "\n",
            "Total Time for Loading Data: 575.6427190303802 seconds\n",
            "Training epoch: 1\n",
            "  9% 8/91 [05:50<52:48, 38.17s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fgu3VvuKwmF",
        "outputId": "47ff8436-e6a2-4214-8215-4565943df32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-14 07:02:30.576175: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "/content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py:19: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  wer_metric = load_metric(\"wer\")\n",
            "/device:GPU:0\n",
            "Model defined âœ… âœ… âœ… âœ…\n",
            "\n",
            "Loading data.....\n",
            "10000 data loaded !!!\n",
            "Data loaded âœ… âœ… âœ… âœ…\n",
            "And It took 58.04688501358032 seconds\n",
            "\n",
            "Cleaning the audio files.....\n",
            "Audio files cleaned âœ… âœ… âœ… âœ…\n",
            "And It took 19.06446361541748 seconds\n",
            "\n",
            "Generating mfcc features.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/training_checkpoints"
      ],
      "metadata": {
        "id": "b3MN3DwcM5ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# with open('/content/drive/MyDrive/Automatic-Nepali-Speech-Recognition-and-Summarizer/ASR/data_asr/vocabulary_asr/new_vocab.json','r',encoding='utf8') as f:\n",
        "#     chars_dict = json.load(f)\n",
        "# unq_chars = list(chars_dict.keys())\n",
        "# unq_chars"
      ],
      "metadata": {
        "id": "NtwErL9vaErC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateWER(actual_label, predicted_label):\n",
        "    # convert string to list\n",
        "    actual_words = actual_label.split()\n",
        "    predicted_words = predicted_label.split()\n",
        "    # costs will hold the costs like in Levenshtein distance algorithm\n",
        "    costs = [[0 for inner in range(len(predicted_words)+1)] for outer in range(len(actual_words)+1)]\n",
        "    # backtrace will hold the operations we've done.\n",
        "    # so we could later backtrace, like the WER algorithm requires us to.\n",
        "    backtrace = [[0 for inner in range(len(predicted_words)+1)] for outer in range(len(actual_words)+1)]\n",
        "    # ok means no change, sub means substitution, ins means insertion and del means deletion\n",
        "    operations = {\n",
        "        'ok': 0,\n",
        "        'sub': 1,\n",
        "        'ins': 2,\n",
        "        'del': 3\n",
        "    }\n",
        "    # penalties for insertion, substitution and deletion\n",
        "    penalties = {\n",
        "        'ins': 1,\n",
        "        'sub': 1,\n",
        "        'del': 1\n",
        "    }\n",
        "    # First column represents the case where we achieve zero predicted labels i-e all the actual labels were deleted \n",
        "    for i in range(1,len(actual_words)+1):\n",
        "        costs[i][0] = penalties['del']*i \n",
        "        backtrace[i][0] = operations['del']\n",
        "    \n",
        "    # First row represents the case where we achieve the predicted label by inserting all the predicted labels into a zero length actual label i-e all unwanted insertions \n",
        "    for j in range(1,len(predicted_words)+1):\n",
        "        costs[0][j] = penalties['ins']*j \n",
        "        backtrace[0][j] = operations['ins']\n",
        "    \n",
        "    # computation\n",
        "    for i in  range(1,len(actual_words)+1):\n",
        "        for j in range(1,len(predicted_words)+1):\n",
        "            # no change in predictions and actual label\n",
        "            if actual_words[i-1] == predicted_words[j-1]:\n",
        "                costs[i][j] = costs[i-1][j-1]\n",
        "                backtrace[i][j] = operations['ok']\n",
        "            else:\n",
        "                # change has occured\n",
        "                sub_cost = costs[i-1][j-1] + penalties['sub']\n",
        "                ins_cost = costs[i][j-1] + penalties['ins']\n",
        "                del_cost = costs[i-1][j] + penalties['del']\n",
        "                costs[i][j] = min(sub_cost,ins_cost,del_cost)\n",
        "                if costs[i][j] == sub_cost:\n",
        "                    backtrace[i][j] = operations['sub']\n",
        "                elif costs[i][j] == ins_cost:\n",
        "                    backtrace[i][j] = operations['ins']\n",
        "                else: \n",
        "                    backtrace[i][j] = operations['del']\n",
        "    \n",
        "    # backtrace through the best route\n",
        "    i = len(actual_words)\n",
        "    j = len(predicted_words)\n",
        "    sub_count = 0 \n",
        "    del_count = 0 \n",
        "    ins_count = 0 \n",
        "    correct_count = 0 \n",
        "\n",
        "    while i > 0 or j > 0:\n",
        "        if backtrace[i][j] == operations['ok']:\n",
        "            correct_count += 1\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif backtrace[i][j] == operations['sub']:\n",
        "            sub_count += 1 \n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif backtrace[i][j] == operations['ins']:\n",
        "            ins_count += 1\n",
        "            j -= 1\n",
        "        elif backtrace[i][j] == operations['del']:\n",
        "            del_count += 1\n",
        "            i -= 1\n",
        "    \n",
        "    \"\"\" \n",
        "    WER formula: \n",
        "    WER = S + D + I / N = S + D I / S + D + C\n",
        "    \"\"\"\n",
        "    wer = round((sub_count + del_count + ins_count)/(sub_count + del_count + correct_count),3)\n",
        "    # wer = round((sub_count + ins_count + del_count)/(float)(len(actual_words)),3)\n",
        "    return wer "
      ],
      "metadata": {
        "id": "My_3_Zubh7NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the WER and CER \n",
        "def calculateErrorRates(actual_label,predicted_label):\n",
        "    # For CER\n",
        "    sm = ed.SequenceMatcher(predicted_label,actual_label)\n",
        "    ed_dist = sm.distance() \n",
        "    cer = ed_dist/len(actual_label)\n",
        "    # For WER \n",
        "    wer = calculateWER(actual_label,predicted_label)\n"
      ],
      "metadata": {
        "id": "V5wtCotp2kLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateErrorRates(actual_label,predicted_label):\n",
        "    # To load the evaluation metrics \n",
        "    wer_metric = load_metric(\"wer\")\n",
        "    cer_metric = load_metric(\"cer\",revision=\"master\")\n",
        "    # Calculate CER and WER for given arguments \n",
        "    cer = cer_metric.compute(predictions=[predicted_label],references=[actual_label])\n",
        "    wer = wer_metric.compute(predictions=[predicted_label],references=[actual_label])\n",
        "    return cer,wer"
      ],
      "metadata": {
        "id": "uIG8GR14iHAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateBatchErrorRates(output,target,start,end,cer,wer):\n",
        "    \"\"\"\n",
        "        The line of codes below is for computing evaluation metric (CER) on internal validation data.\n",
        "    \"\"\"\n",
        "    input_len = np.ones(output.shape[0]) * output.shape[1]\n",
        "    # Decode the output using beam search and CTC to get  the required logits\n",
        "    decoded_indices = K.ctc_decode(output, input_length=input_len,\n",
        "                            greedy=False, beam_width=100)[0][0]\n",
        "    \n",
        "    # Remove the padding token from batchified target texts\n",
        "    target_indices = [sent[sent != 0].tolist() for sent in target]\n",
        "\n",
        "    # Remove the padding, unknown token, and blank token from predicted texts\n",
        "    predicted_indices = [sent[sent > 1].numpy().tolist() for sent in decoded_indices] # idx 0: padding token, idx 1: unknown, idx -1: blank token\n",
        "\n",
        "    batch_cer = cer\n",
        "    batch_wer = wer\n",
        "    len_batch = end - start\n",
        "    for i in range(len_batch):\n",
        "        predicted_label = predicted_indices[i]\n",
        "        actual_label = target_indices[i]\n",
        "        error_rates = calculateErrorRates(actual_label,predicted_label)\n",
        "        batch_cer += error_rates[0]\n",
        "        batch_wer += error_rates[1]\n",
        "    batch_cer /= len_batch\n",
        "    batch_wer /= len_batch\n",
        "    return batch_cer,batch_wer"
      ],
      "metadata": {
        "id": "rNhYVbfek9px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, train_wavs, train_texts, validation_wavs, validation_texts, epochs=100, batch_size=50):\n",
        "\n",
        "    with tf.device(device_name):\n",
        "        # These will be the final results to be returned\n",
        "        train_losses = []\n",
        "        validation_losses = []\n",
        "        train_CERs = []\n",
        "        train_WERs = []\n",
        "        validation_CERs = []\n",
        "        validation_WERs = []\n",
        "        for e in range(0, epochs):\n",
        "            start_time = time.time()\n",
        "            len_train = len(train_wavs)\n",
        "            len_validation = len(validation_wavs)\n",
        "            train_loss = 0\n",
        "            training_CER = 0\n",
        "            training_WER = 0\n",
        "            validation_loss = 0\n",
        "            validation_CER = 0\n",
        "            validation_WER = 0\n",
        "            train_batch_count = 0\n",
        "            validation_batch_count = 0\n",
        "            # Training Steps\n",
        "            print(\"Training epoch: {}\".format(e+1))\n",
        "            for start in tqdm(range(0, len_train, batch_size)):\n",
        "\n",
        "                end = None\n",
        "                if start + batch_size < len_train:\n",
        "                    end = start + batch_size\n",
        "                else:\n",
        "                    end = len_train\n",
        "                x, target, target_lengths, output_lengths = batchify(\n",
        "                    train_wavs[start:end], train_texts[start:end], UNQ_CHARS)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    output = model(x, training=True)\n",
        "\n",
        "                    loss = K.ctc_batch_cost(\n",
        "                        target, output, output_lengths, target_lengths)\n",
        "\n",
        "                grads = tape.gradient(loss, model.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "                train_loss += np.average(loss.numpy())\n",
        "                train_batch_count += 1\n",
        "                training_cer, training_wer = calculateBatchErrorRates(output,target,start,end,training_CER,training_WER)\n",
        "\n",
        "\n",
        "            # Validation Step\n",
        "            print(\"Validation epoch: {}\".format(e+1))\n",
        "            for start in tqdm(range(0, len_validation, batch_size)):\n",
        "\n",
        "                end = None\n",
        "                if start + batch_size < len_validation:\n",
        "                    end = start + batch_size\n",
        "                else:\n",
        "                    end = len_validation\n",
        "                x, target, target_lengths, output_lengths = batchify(\n",
        "                    validation_wavs[start:end], validation_texts[start:end], UNQ_CHARS)\n",
        "\n",
        "                output = model(x, training=False)\n",
        "\n",
        "                # Calculate CTC Loss\n",
        "                loss = K.ctc_batch_cost(\n",
        "                    target, output, output_lengths, target_lengths)\n",
        "\n",
        "                validation_loss += np.average(loss.numpy())\n",
        "                validation_batch_count += 1\n",
        "                validation_cer, validation_wer = calculateBatchErrorRates(output,target,start,end,validation_CER,validation_WER)\n",
        "\n",
        "            # Average the results\n",
        "            # losses\n",
        "            train_loss /= train_batch_count\n",
        "            validation_loss /= validation_batch_count\n",
        "            # cers\n",
        "            train_CER /= train_batch_count\n",
        "            validation_CER /= validation_batch_count\n",
        "            # wers \n",
        "            train_WER /= train_batch_count \n",
        "            validation_WER /= validation_batch_count\n",
        "\n",
        "            # Append the results \n",
        "            train_losses.append(train_loss)\n",
        "            train_CERs.append(train_CER)\n",
        "            train_WERs.append(train_WER)\n",
        "            validation_losses.append(validation_loss)\n",
        "            validation_CERs.append(validation_CER)\n",
        "            validation_WERs.append(validation_WER)\n",
        "            \n",
        "            rec = f\"Epoch: {e+1}, Train Loss: {train_loss:.2f}, Validation Loss: {validation_loss:.2f}, Train CER: {(train_CER*100):.2f}, Validation CER: {(validation_CER*100):.2f}, Train WER: {(train_WER*100):.2f}, Validation WER: {(validation_WER*100):.2f} in {(time.time() - start_time):.2f} secs\\n\"\n",
        "\n",
        "            print(rec)\n",
        "\n",
        "        result = {\n",
        "            'epochs': range(0,epochs),\n",
        "            'train_loss': train_losses,\n",
        "            'validation_loss': validation_losses,\n",
        "            'train_cer': train_CERs,\n",
        "            'validation_cer': validation_CERs,\n",
        "            'train_wer': train_WERs,\n",
        "            'validation_wer': validation_WERs\n",
        "        }\n",
        "    \n",
        "    return model, result"
      ],
      "metadata": {
        "id": "OAndIGiOhHUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_model(model, optimizer, train_wavs, train_texts, test_wavs, test_texts, epochs=100, batch_size=50):\n",
        "\n",
        "#     with tf.device(device_name):\n",
        "#         train_losses = []\n",
        "#         validation_losses = []\n",
        "#         train_CERs = []\n",
        "#         train_WERs = []\n",
        "#         validation_CERs = []\n",
        "#         validation_WERs = []\n",
        "#         for e in range(0, epochs):\n",
        "#             start_time = time.time()\n",
        "\n",
        "#             len_train = len(train_wavs)\n",
        "#             len_test = len(test_wavs)\n",
        "#             train_loss = 0\n",
        "#             test_loss = 0\n",
        "#             test_CER = 0\n",
        "#             train_batch_count = 0\n",
        "#             test_batch_count = 0\n",
        "\n",
        "#             print(\"Training epoch: {}\".format(e+1))\n",
        "#             for start in tqdm(range(0, len_train, batch_size)):\n",
        "\n",
        "#                 end = None\n",
        "#                 if start + batch_size < len_train:\n",
        "#                     end = start + batch_size\n",
        "#                 else:\n",
        "#                     end = len_train\n",
        "#                 x, target, target_lengths, output_lengths = batchify(\n",
        "#                     train_wavs[start:end], train_texts[start:end], UNQ_CHARS)\n",
        "\n",
        "#                 with tf.GradientTape() as tape:\n",
        "#                     output = model(x, training=True)\n",
        "\n",
        "#                     loss = K.ctc_batch_cost(\n",
        "#                         target, output, output_lengths, target_lengths)\n",
        "\n",
        "#                 grads = tape.gradient(loss, model.trainable_weights)\n",
        "#                 optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "#                 train_loss += np.average(loss.numpy())\n",
        "#                 train_batch_count += 1\n",
        "\n",
        "#             print(\"Testing epoch: {}\".format(e+1))\n",
        "#             for start in tqdm(range(0, len_test, batch_size)):\n",
        "\n",
        "#                 end = None\n",
        "#                 if start + batch_size < len_test:\n",
        "#                     end = start + batch_size\n",
        "#                 else:\n",
        "#                     end = len_test\n",
        "#                 x, target, target_lengths, output_lengths = batchify(\n",
        "#                     test_wavs[start:end], test_texts[start:end], UNQ_CHARS)\n",
        "\n",
        "#                 output = model(x, training=False)\n",
        "\n",
        "#                 # Calculate CTC Loss\n",
        "#                 loss = K.ctc_batch_cost(\n",
        "#                     target, output, output_lengths, target_lengths)\n",
        "\n",
        "#                 test_loss += np.average(loss.numpy())\n",
        "#                 test_batch_count += 1\n",
        "\n",
        "#                 \"\"\"\n",
        "#                     The line of codes below is for computing evaluation metric (CER) on internal validation data.\n",
        "#                 \"\"\"\n",
        "#                 input_len = np.ones(output.shape[0]) * output.shape[1]\n",
        "#                 decoded_indices = K.ctc_decode(output, input_length=input_len,\n",
        "#                                        greedy=False, beam_width=100)[0][0]\n",
        "                \n",
        "#                 # Remove the padding token from batchified target texts\n",
        "#                 target_indices = [sent[sent != 0].tolist() for sent in target]\n",
        "\n",
        "#                 # Remove the padding, unknown token, and blank token from predicted texts\n",
        "#                 predicted_indices = [sent[sent > 1].numpy().tolist() for sent in decoded_indices] # idx 0: padding token, idx 1: unknown, idx -1: blank token\n",
        "\n",
        "#                 len_batch = end - start\n",
        "#                 for i in range(len_batch):\n",
        "\n",
        "#                     pred = predicted_indices[i]\n",
        "#                     truth = target_indices[i]\n",
        "#                     sm = ed.SequenceMatcher(pred, truth)\n",
        "#                     ed_dist = sm.distance()                 # Edit distance\n",
        "#                     test_CER += ed_dist / len(truth)\n",
        "#                 test_CER /= len_batch\n",
        "\n",
        "#             train_loss /= train_batch_count\n",
        "#             test_loss /= test_batch_count\n",
        "#             test_CER /= test_batch_count\n",
        "\n",
        "#             rec = \"Epoch: {}, Train Loss: {:.2f}, Test Loss {:.2f}, Test CER {:.2f} % in {:.2f} secs.\\n\".format(\n",
        "#                 e+1, train_loss, test_loss, test_CER*100, time.time() - start_time)\n",
        "\n",
        "#             print(rec)\n",
        "#         return model"
      ],
      "metadata": {
        "id": "b5H79KizE-jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "len(os.listdir('/content/audio'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVZJLy2ZBVAq",
        "outputId": "e36b1ddb-3892-4172-d512-b32f46f1b891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142695"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-1qD2G9Yg_L",
        "outputId": "1a0b5049-ad3e-4137-9767-80796cb9ac10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-13 16:59:00.181764: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "/content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/trainer.py:18: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  wer_metric = load_metric(\"wer\")\n",
            "Model defined âœ… âœ… âœ… âœ…\n",
            "\n",
            "Loading data.....\n",
            "Data loaded âœ… âœ… âœ… âœ…\n",
            "\n",
            "Cleaning the audio files.....\n",
            "Audio files cleaned âœ… âœ… âœ… âœ…\n",
            "\n",
            "Generating mfcc features.....\n",
            "MFCC features generated âœ… âœ… âœ… âœ…\n",
            "\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Training epoch: 1\n",
            "100% 4/4 [00:10<00:00,  2.61s/it]\n",
            "Validation epoch: 1\n",
            "100% 1/1 [00:01<00:00,  1.31s/it]\n",
            "Epoch: 1, Train Loss: 1280.79, Validation Loss: 1693.57, Train CER: 25.54, Validation CER: 82.85, Train WER: 41.78, Validation WER: 93.38 in 11.76 secs\n",
            "\n",
            "Training epoch: 2\n",
            "100% 4/4 [00:06<00:00,  1.56s/it]\n",
            "Validation epoch: 2\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 2, Train Loss: 1011.85, Validation Loss: 749.94, Train CER: 39.82, Validation CER: 99.43, Train WER: 46.56, Validation WER: 100.00 in 7.43 secs\n",
            "\n",
            "Training epoch: 3\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 3\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 3, Train Loss: 752.52, Validation Loss: 703.05, Train CER: 33.76, Validation CER: 99.43, Train WER: 40.59, Validation WER: 100.00 in 7.11 secs\n",
            "\n",
            "Training epoch: 4\n",
            "100% 4/4 [00:06<00:00,  1.58s/it]\n",
            "Validation epoch: 4\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 4, Train Loss: 790.66, Validation Loss: 752.43, Train CER: 29.48, Validation CER: 99.43, Train WER: 48.76, Validation WER: 100.00 in 7.54 secs\n",
            "\n",
            "Training epoch: 5\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 5\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 5, Train Loss: 611.73, Validation Loss: 847.54, Train CER: 35.95, Validation CER: 99.62, Train WER: 42.20, Validation WER: 100.00 in 7.16 secs\n",
            "\n",
            "Training epoch: 6\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 6\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 6, Train Loss: 603.56, Validation Loss: 728.63, Train CER: 36.95, Validation CER: 99.33, Train WER: 40.29, Validation WER: 100.00 in 7.00 secs\n",
            "\n",
            "Training epoch: 7\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 7\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 7, Train Loss: 566.74, Validation Loss: 616.69, Train CER: 35.16, Validation CER: 99.33, Train WER: 39.74, Validation WER: 100.00 in 7.10 secs\n",
            "\n",
            "Training epoch: 8\n",
            "100% 4/4 [00:06<00:00,  1.50s/it]\n",
            "Validation epoch: 8\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n",
            "Epoch: 8, Train Loss: 628.30, Validation Loss: 554.44, Train CER: 33.27, Validation CER: 99.05, Train WER: 39.26, Validation WER: 100.00 in 7.26 secs\n",
            "\n",
            "Training epoch: 9\n",
            "100% 4/4 [00:07<00:00,  1.92s/it]\n",
            "Validation epoch: 9\n",
            "100% 1/1 [00:01<00:00,  1.29s/it]\n",
            "Epoch: 9, Train Loss: 557.66, Validation Loss: 491.23, Train CER: 34.49, Validation CER: 99.43, Train WER: 39.72, Validation WER: 100.00 in 8.99 secs\n",
            "\n",
            "Training epoch: 10\n",
            "100% 4/4 [00:06<00:00,  1.52s/it]\n",
            "Validation epoch: 10\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 10, Train Loss: 536.00, Validation Loss: 475.31, Train CER: 36.02, Validation CER: 99.43, Train WER: 40.00, Validation WER: 100.00 in 7.31 secs\n",
            "\n",
            "Training epoch: 11\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 11\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 11, Train Loss: 519.79, Validation Loss: 515.63, Train CER: 38.59, Validation CER: 99.43, Train WER: 40.73, Validation WER: 100.00 in 7.07 secs\n",
            "\n",
            "Training epoch: 12\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 12\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 12, Train Loss: 510.44, Validation Loss: 519.98, Train CER: 38.74, Validation CER: 99.05, Train WER: 41.12, Validation WER: 100.00 in 7.03 secs\n",
            "\n",
            "Training epoch: 13\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 13\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 13, Train Loss: 501.77, Validation Loss: 528.24, Train CER: 36.68, Validation CER: 99.05, Train WER: 39.97, Validation WER: 100.00 in 7.14 secs\n",
            "\n",
            "Training epoch: 14\n",
            "100% 4/4 [00:06<00:00,  1.50s/it]\n",
            "Validation epoch: 14\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 14, Train Loss: 494.66, Validation Loss: 548.36, Train CER: 37.03, Validation CER: 99.43, Train WER: 40.01, Validation WER: 100.00 in 7.24 secs\n",
            "\n",
            "Training epoch: 15\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 15\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 15, Train Loss: 489.09, Validation Loss: 532.12, Train CER: 36.59, Validation CER: 99.43, Train WER: 40.20, Validation WER: 100.00 in 7.14 secs\n",
            "\n",
            "Training epoch: 16\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 16\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 16, Train Loss: 484.91, Validation Loss: 520.51, Train CER: 36.38, Validation CER: 99.43, Train WER: 40.15, Validation WER: 100.00 in 7.07 secs\n",
            "\n",
            "Training epoch: 17\n",
            "100% 4/4 [00:07<00:00,  1.88s/it]\n",
            "Validation epoch: 17\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 17, Train Loss: 481.24, Validation Loss: 510.46, Train CER: 36.51, Validation CER: 98.00, Train WER: 39.79, Validation WER: 99.26 in 8.73 secs\n",
            "\n",
            "Training epoch: 18\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 18\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 18, Train Loss: 477.13, Validation Loss: 492.65, Train CER: 36.41, Validation CER: 97.34, Train WER: 39.72, Validation WER: 98.53 in 7.13 secs\n",
            "\n",
            "Training epoch: 19\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 19\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 19, Train Loss: 474.36, Validation Loss: 502.72, Train CER: 36.44, Validation CER: 96.56, Train WER: 39.43, Validation WER: 97.79 in 7.08 secs\n",
            "\n",
            "Training epoch: 20\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 20\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n",
            "Epoch: 20, Train Loss: 471.21, Validation Loss: 543.75, Train CER: 36.43, Validation CER: 96.85, Train WER: 39.27, Validation WER: 98.16 in 7.06 secs\n",
            "\n",
            "Training epoch: 21\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 21\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 21, Train Loss: 468.74, Validation Loss: 571.98, Train CER: 36.11, Validation CER: 94.08, Train WER: 39.14, Validation WER: 95.59 in 7.09 secs\n",
            "\n",
            "Training epoch: 22\n",
            "100% 4/4 [00:05<00:00,  1.49s/it]\n",
            "Validation epoch: 22\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 22, Train Loss: 466.38, Validation Loss: 602.82, Train CER: 36.33, Validation CER: 95.12, Train WER: 39.56, Validation WER: 96.32 in 7.17 secs\n",
            "\n",
            "Training epoch: 23\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 23\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 23, Train Loss: 463.38, Validation Loss: 589.95, Train CER: 36.17, Validation CER: 94.45, Train WER: 39.30, Validation WER: 95.59 in 7.07 secs\n",
            "\n",
            "Training epoch: 24\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 24\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n",
            "Epoch: 24, Train Loss: 460.56, Validation Loss: 605.70, Train CER: 36.21, Validation CER: 93.01, Train WER: 39.12, Validation WER: 94.85 in 7.10 secs\n",
            "\n",
            "Training epoch: 25\n",
            "100% 4/4 [00:07<00:00,  1.90s/it]\n",
            "Validation epoch: 25\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 25, Train Loss: 459.02, Validation Loss: 643.99, Train CER: 35.79, Validation CER: 92.44, Train WER: 38.76, Validation WER: 94.12 in 8.87 secs\n",
            "\n",
            "Training epoch: 26\n",
            "100% 4/4 [00:06<00:00,  1.50s/it]\n",
            "Validation epoch: 26\n",
            "100% 1/1 [00:01<00:00,  1.28s/it]\n",
            "Epoch: 26, Train Loss: 456.72, Validation Loss: 786.98, Train CER: 36.02, Validation CER: 93.20, Train WER: 39.17, Validation WER: 94.12 in 7.29 secs\n",
            "\n",
            "Training epoch: 27\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 27\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 27, Train Loss: 453.91, Validation Loss: 787.60, Train CER: 36.03, Validation CER: 92.53, Train WER: 39.05, Validation WER: 93.75 in 7.13 secs\n",
            "\n",
            "Training epoch: 28\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 28\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 28, Train Loss: 451.39, Validation Loss: 687.75, Train CER: 35.63, Validation CER: 92.15, Train WER: 38.52, Validation WER: 93.75 in 7.04 secs\n",
            "\n",
            "Training epoch: 29\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 29\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 29, Train Loss: 449.32, Validation Loss: 637.04, Train CER: 35.79, Validation CER: 92.15, Train WER: 38.65, Validation WER: 93.75 in 7.10 secs\n",
            "\n",
            "Training epoch: 30\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 30\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 30, Train Loss: 446.64, Validation Loss: 593.74, Train CER: 36.32, Validation CER: 91.19, Train WER: 39.27, Validation WER: 93.01 in 7.13 secs\n",
            "\n",
            "Training epoch: 31\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 31\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 31, Train Loss: 443.89, Validation Loss: 590.29, Train CER: 35.69, Validation CER: 91.10, Train WER: 38.86, Validation WER: 93.01 in 7.09 secs\n",
            "\n",
            "Training epoch: 32\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 32\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 32, Train Loss: 441.90, Validation Loss: 579.92, Train CER: 35.89, Validation CER: 90.52, Train WER: 38.75, Validation WER: 92.65 in 7.07 secs\n",
            "\n",
            "Training epoch: 33\n",
            "100% 4/4 [00:07<00:00,  1.88s/it]\n",
            "Validation epoch: 33\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 33, Train Loss: 445.13, Validation Loss: 577.09, Train CER: 35.25, Validation CER: 89.18, Train WER: 38.28, Validation WER: 91.91 in 8.77 secs\n",
            "\n",
            "Training epoch: 34\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 34\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n",
            "Epoch: 34, Train Loss: 439.25, Validation Loss: 666.87, Train CER: 35.26, Validation CER: 88.22, Train WER: 38.74, Validation WER: 91.91 in 7.15 secs\n",
            "\n",
            "Training epoch: 35\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 35\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 35, Train Loss: 439.19, Validation Loss: 724.58, Train CER: 35.29, Validation CER: 88.70, Train WER: 38.48, Validation WER: 91.54 in 7.19 secs\n",
            "\n",
            "Training epoch: 36\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 36\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 36, Train Loss: 435.62, Validation Loss: 721.14, Train CER: 35.54, Validation CER: 87.36, Train WER: 38.26, Validation WER: 91.18 in 7.11 secs\n",
            "\n",
            "Training epoch: 37\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 37\n",
            "100% 1/1 [00:01<00:00,  1.20s/it]\n",
            "Epoch: 37, Train Loss: 431.94, Validation Loss: 707.94, Train CER: 34.69, Validation CER: 87.07, Train WER: 38.19, Validation WER: 91.54 in 7.06 secs\n",
            "\n",
            "Training epoch: 38\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 38\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 38, Train Loss: 431.12, Validation Loss: 747.13, Train CER: 35.51, Validation CER: 87.74, Train WER: 38.56, Validation WER: 91.54 in 7.08 secs\n",
            "\n",
            "Training epoch: 39\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 39\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 39, Train Loss: 427.91, Validation Loss: 769.90, Train CER: 34.64, Validation CER: 87.07, Train WER: 37.99, Validation WER: 91.54 in 7.05 secs\n",
            "\n",
            "Training epoch: 40\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 40\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 40, Train Loss: 424.92, Validation Loss: 781.29, Train CER: 34.85, Validation CER: 86.31, Train WER: 38.23, Validation WER: 91.54 in 7.14 secs\n",
            "\n",
            "Training epoch: 41\n",
            "100% 4/4 [00:07<00:00,  1.88s/it]\n",
            "Validation epoch: 41\n",
            "100% 1/1 [00:01<00:00,  1.27s/it]\n",
            "Epoch: 41, Train Loss: 422.30, Validation Loss: 783.61, Train CER: 35.09, Validation CER: 86.79, Train WER: 38.14, Validation WER: 91.54 in 8.78 secs\n",
            "\n",
            "Training epoch: 42\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 42\n",
            "100% 1/1 [00:01<00:00,  1.25s/it]\n",
            "Epoch: 42, Train Loss: 418.24, Validation Loss: 775.93, Train CER: 34.98, Validation CER: 86.50, Train WER: 38.15, Validation WER: 91.18 in 7.13 secs\n",
            "\n",
            "Training epoch: 43\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 43\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 43, Train Loss: 416.24, Validation Loss: 780.63, Train CER: 34.68, Validation CER: 85.06, Train WER: 37.74, Validation WER: 90.44 in 7.09 secs\n",
            "\n",
            "Training epoch: 44\n",
            "100% 4/4 [00:05<00:00,  1.47s/it]\n",
            "Validation epoch: 44\n",
            "100% 1/1 [00:01<00:00,  1.19s/it]\n",
            "Epoch: 44, Train Loss: 413.20, Validation Loss: 805.22, Train CER: 34.02, Validation CER: 86.79, Train WER: 37.39, Validation WER: 91.54 in 7.09 secs\n",
            "\n",
            "Training epoch: 45\n",
            "100% 4/4 [00:05<00:00,  1.46s/it]\n",
            "Validation epoch: 45\n",
            "100% 1/1 [00:01<00:00,  1.21s/it]\n",
            "Epoch: 45, Train Loss: 410.66, Validation Loss: 802.48, Train CER: 34.88, Validation CER: 86.12, Train WER: 38.13, Validation WER: 91.91 in 7.06 secs\n",
            "\n",
            "Training epoch: 46\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 46\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 46, Train Loss: 408.34, Validation Loss: 798.20, Train CER: 34.65, Validation CER: 86.88, Train WER: 37.68, Validation WER: 91.91 in 7.16 secs\n",
            "\n",
            "Training epoch: 47\n",
            "100% 4/4 [00:05<00:00,  1.49s/it]\n",
            "Validation epoch: 47\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 47, Train Loss: 407.72, Validation Loss: 814.06, Train CER: 33.90, Validation CER: 87.17, Train WER: 36.71, Validation WER: 91.54 in 7.18 secs\n",
            "\n",
            "Training epoch: 48\n",
            "100% 4/4 [00:06<00:00,  1.50s/it]\n",
            "Validation epoch: 48\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 48, Train Loss: 402.29, Validation Loss: 821.85, Train CER: 34.28, Validation CER: 85.83, Train WER: 37.16, Validation WER: 90.81 in 7.23 secs\n",
            "\n",
            "Training epoch: 49\n",
            "100% 4/4 [00:06<00:00,  1.73s/it]\n",
            "Validation epoch: 49\n",
            "100% 1/1 [00:01<00:00,  1.91s/it]\n",
            "Epoch: 49, Train Loss: 400.22, Validation Loss: 822.76, Train CER: 35.11, Validation CER: 87.74, Train WER: 37.69, Validation WER: 92.65 in 8.81 secs\n",
            "\n",
            "Training epoch: 50\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 50\n",
            "100% 1/1 [00:01<00:00,  1.22s/it]\n",
            "Epoch: 50, Train Loss: 396.92, Validation Loss: 829.29, Train CER: 33.67, Validation CER: 84.58, Train WER: 37.21, Validation WER: 92.28 in 7.13 secs\n",
            "\n",
            "Training epoch: 51\n",
            "100% 4/4 [00:05<00:00,  1.49s/it]\n",
            "Validation epoch: 51\n",
            "100% 1/1 [00:01<00:00,  1.23s/it]\n",
            "Epoch: 51, Train Loss: 392.96, Validation Loss: 847.86, Train CER: 33.25, Validation CER: 85.83, Train WER: 36.41, Validation WER: 90.44 in 7.18 secs\n",
            "\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Checkpoint Saved\n",
            "Training epoch: 52\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 52\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 52, Train Loss: 390.42, Validation Loss: 860.62, Train CER: 34.98, Validation CER: 86.02, Train WER: 37.45, Validation WER: 91.54 in 7.15 secs\n",
            "\n",
            "Training epoch: 53\n",
            "100% 4/4 [00:05<00:00,  1.45s/it]\n",
            "Validation epoch: 53\n",
            "100% 1/1 [00:01<00:00,  1.26s/it]\n",
            "Epoch: 53, Train Loss: 386.09, Validation Loss: 854.74, Train CER: 33.60, Validation CER: 84.68, Train WER: 36.27, Validation WER: 90.07 in 7.06 secs\n",
            "\n",
            "Training epoch: 54\n",
            "100% 4/4 [00:06<00:00,  1.61s/it]\n",
            "Validation epoch: 54\n",
            "100% 1/1 [00:01<00:00,  1.33s/it]\n",
            "Epoch: 54, Train Loss: 384.71, Validation Loss: 843.70, Train CER: 33.03, Validation CER: 83.15, Train WER: 36.62, Validation WER: 90.81 in 7.76 secs\n",
            "\n",
            "Training epoch: 55\n",
            "100% 4/4 [00:06<00:00,  1.73s/it]\n",
            "Validation epoch: 55\n",
            "100% 1/1 [00:01<00:00,  1.80s/it]\n",
            "Epoch: 55, Train Loss: 380.96, Validation Loss: 856.30, Train CER: 34.68, Validation CER: 83.24, Train WER: 37.16, Validation WER: 91.18 in 8.71 secs\n",
            "\n",
            "Training epoch: 56\n",
            "100% 4/4 [00:06<00:00,  1.66s/it]\n",
            "Validation epoch: 56\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 56, Train Loss: 375.76, Validation Loss: 869.86, Train CER: 33.26, Validation CER: 83.82, Train WER: 36.13, Validation WER: 90.81 in 7.86 secs\n",
            "\n",
            "Training epoch: 57\n",
            "100% 4/4 [00:07<00:00,  1.95s/it]\n",
            "Validation epoch: 57\n",
            "100% 1/1 [00:01<00:00,  1.24s/it]\n",
            "Epoch: 57, Train Loss: 372.25, Validation Loss: 857.54, Train CER: 31.78, Validation CER: 82.00, Train WER: 35.32, Validation WER: 90.07 in 9.04 secs\n",
            "\n",
            "Training epoch: 58\n",
            "100% 4/4 [00:05<00:00,  1.48s/it]\n",
            "Validation epoch: 58\n",
            "100% 1/1 [00:01<00:00,  1.28s/it]\n",
            "Epoch: 58, Train Loss: 370.01, Validation Loss: 830.14, Train CER: 33.89, Validation CER: 83.34, Train WER: 36.51, Validation WER: 91.18 in 7.21 secs\n",
            "\n",
            "Training epoch: 59\n",
            "100% 4/4 [00:07<00:00,  1.92s/it]\n",
            "Validation epoch: 59\n",
            "100% 1/1 [00:02<00:00,  2.02s/it]\n",
            "Epoch: 59, Train Loss: 368.12, Validation Loss: 862.22, Train CER: 32.59, Validation CER: 84.01, Train WER: 35.72, Validation WER: 92.28 in 9.69 secs\n",
            "\n",
            "Training epoch: 60\n",
            "100% 4/4 [00:09<00:00,  2.29s/it]\n",
            "Validation epoch: 60\n",
            "100% 1/1 [00:01<00:00,  1.99s/it]\n",
            "Epoch: 60, Train Loss: 366.66, Validation Loss: 890.17, Train CER: 33.07, Validation CER: 82.48, Train WER: 36.10, Validation WER: 90.07 in 11.15 secs\n",
            "\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Model Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import pandas as pd "
      ],
      "metadata": {
        "id": "XwnxyMvMcYub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_df = pd.read_csv('/content/drive/MyDrive/Automatic-Nepali-Speech-Recognition-and-Summarizer/ASR/data_asr/transcript_asr/new_transcript_for_asr_complete.csv')[:10]\n",
        "train_wavs = []\n",
        "for f_name in texts_df[\"filename\"]:\n",
        "    wav, _ = librosa.load(f\"/content/audio/audio/{f_name}.flac\", sr=16000)\n",
        "    train_wavs.append(wav)\n",
        "train_texts = texts_df[\"label\"].tolist()\n",
        "print(train_wavs[-1],train_texts[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maCdGG2nZQoq",
        "outputId": "28c6da3c-bea0-4164-e1a8-13bd679fbe61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.9156721e-05  1.8241990e-05  3.8159305e-06 ... -8.6012187e-07\n",
            "  7.8891588e-07 -6.7354171e-07] à¤†à¤¨à¥à¤¤à¤°à¤¿à¤• à¤°à¤¾à¤œà¤¸à¥à¤µ à¤•à¤¾à¤°à¥à¤¯à¤¾à¤²à¤¯ à¤­à¥ˆà¤°à¤¹à¤µà¤¾ à¤…à¤¨à¥à¤¤à¤°à¥à¤—à¤¤ à¤°à¥à¤ªà¤¨à¥à¤¦à¥‡à¤¹à¥€à¤•à¥‹ à¤¸à¤¿à¤¦à¥à¤§à¤¾à¤°à¥à¤¥à¤¨à¤—à¤° à¤¨à¤—à¤°à¤ªà¤¾à¤²à¤¿à¤•à¤¾ à¤²à¥à¤®à¥à¤¬à¤¿à¤¨à¥€ à¤¨à¤—à¤°à¤ªà¤¾à¤²à¤¿à¤•à¤¾ à¤¤à¤¿à¤²à¥‹à¤¤à¥à¤¤à¤®à¤¾ à¤¨à¤—à¤°à¤ªà¤¾à¤²à¤¿à¤•à¤¾à¤®à¤¾ à¤®à¥‚à¤²à¥à¤¯ à¤…à¤­à¤¿à¤µà¥ƒà¤¦à¥à¤§à¤¿ à¤•à¤°à¤®à¤¾ à¤¦à¤°à¥à¤¤à¤¾ à¤›\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/ASR-Nepali-using-CNN-BiLSTM-ResNet/eval.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZhEyaeZckly",
        "outputId": "73843f2e-097a-4b57-9485-d224d8e7cfee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-11 16:33:13.369498: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Loading model.....\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model loaded âœ… âœ… âœ… âœ…\n",
            "\n",
            "Loading wav files.....\n",
            "Wav files loaded âœ… âœ… âœ… âœ…\n",
            "\n",
            "Predicting sentences.....\n",
            "['à¤°à¥à¥‹à¤°à¤¨à¤¾à¤ªà¤›à¤¿ à¤šà¤²à¤šà¤¿à¤¤à¥à¤°à¤•à¥‹ à¤•à¤¥à¤¾à¤•à¥‹', 'à¤œà¥‹ à¤‡ à¤¸à¥à¤²à¤¾à¤®à¤•à¥‹ à¤ªà¤¹à¤¿à¤²à¥‹'] \n",
            "\n",
            "Calculating CER.....\n",
            "CER -> 13.29%, \t No.of sentences -> 2, \t Time Taken -> 2.15 secs.\n",
            "The total time taken for all sentences CER calculation is  2.15 secs.\n",
            "0.1328976034858388 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZMsyZ5SiKNJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}